{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd3921d",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1346ff",
   "metadata": {},
   "source": [
    "### 1. 모델 로드 및 평가 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e334d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeeho/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/jeeho/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, BitsAndBytesConfig\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "## Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "## 모델과 토크나이저의 경로\n",
    "model_path = './checkpoints/checkpoint-16000'\n",
    "\n",
    "## 모델과 토크나이저 로드\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2093392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                            dialect  \\\n",
      "0               0  제사를 모시고 있는데 할머니 할아버지 제사까지 다 모시고 자식 없는 큰 아비 제사까...   \n",
      "1               1  그랑께 메느리가 교회 권사가 돼갖고 안 하려고 하는데 어찌게 집 안에서 이래라 저래...   \n",
      "2               2    그랑께 내가 해놓고 제가 와서 기도나 하고 그러라고 가정 편하게 하려고 그라고 삽니다   \n",
      "3              17  봄 되면 친구들이랑 꽃구경 자주 가지요 삼월달에도 가고 사월달에 사월 오일이면 벚꽃...   \n",
      "4              20  그전에는 처음에는 아기들을 시누이들 살고 같이 살 때는 한 번씩 했는데 지금은 화투...   \n",
      "...           ...                                                ...   \n",
      "36692       67492               내장산 선운사 관악산 월출산 어 주로 가을에 단풍놀이를 많이 가제   \n",
      "36693       67493                          나는 여그 읍에서 이 도시에서 사는 거이 좋소   \n",
      "36694       67494  시골에 삼스로 부잣집 시집 와 갖고 어찌께 일이 많은가라 논밭이 많은가 어른은 일을...   \n",
      "36695       67495                        그랑께 지금은 참 대통령 시상 살고 있소 편안하니   \n",
      "36696       67496             그랑께 나는 이 도시가 좋소야 너무 고생해서 일을 못 해갖고 고생해서   \n",
      "\n",
      "                                                standard  form_len  \n",
      "0      제사를 모시고 있는데 할머니 할아버지 제사까지 다 모시고 자식 없는 큰 아비 제사까...       141  \n",
      "1      그러니까 며느리가 교회 권사가 돼갖고 안 하려고 하는데 어떻게 집 안에서 이래라 저...        62  \n",
      "2        그랑께 내가 해놓고 제가 와서 기도나 하고 그러라고 가정 편하게 하려고 그러고 삽니다        47  \n",
      "3      봄 되면 친구들이랑 꽃구경 자주 가지요 삼월달에도 가고 사월달에 사월 오일이면 벚꽃...        68  \n",
      "4      그전에는 처음에는 아기들을 시누이들 살고 같이 살 때는 한 번씩 했는데 지금은 화투...        91  \n",
      "...                                                  ...       ...  \n",
      "36692               내장산 선운사 관악산 월출산 어 주로 가을에 단풍놀이를 많이 가지        36  \n",
      "36693                          나는 여기 읍에서 이 도시에서 사는 거이 좋소        25  \n",
      "36694  시골에 살면서 부잣집 시집 와 갖고 어찌 일이 많은가라 논밭이 많은가 어른은 일을 ...        71  \n",
      "36695                       그러니까 지금은 참 대통령 세상 살고 있소 편안하니        28  \n",
      "36696           그러니까 나는 이 도시가 좋아요  너무 고생해서 일을 못 해갖고 고생해서        40  \n",
      "\n",
      "[36697 rows x 4 columns]\n",
      "       Unnamed: 0                                            dialect  \\\n",
      "1               1  그랑께 메느리가 교회 권사가 돼갖고 안 하려고 하는데 어찌게 집 안에서 이래라 저래...   \n",
      "2               2    그랑께 내가 해놓고 제가 와서 기도나 하고 그러라고 가정 편하게 하려고 그라고 삽니다   \n",
      "7              25                                       윷 놀이도 허고 그래요   \n",
      "8              31       뭐 그냥 세 살 네 살 애들도 갖고 헐 만한 판이 있더만 뭐판인지 모르는데 있어   \n",
      "12             35  그랑게 될 수 있으면 한 나이도 또 참 그것도 나이가 묵으면 헷갈릴 때도 있고 여러...   \n",
      "...           ...                                                ...   \n",
      "36691       67491                                        단풍놀이는 많이 갔제   \n",
      "36692       67492               내장산 선운사 관악산 월출산 어 주로 가을에 단풍놀이를 많이 가제   \n",
      "36693       67493                          나는 여그 읍에서 이 도시에서 사는 거이 좋소   \n",
      "36695       67495                        그랑께 지금은 참 대통령 시상 살고 있소 편안하니   \n",
      "36696       67496             그랑께 나는 이 도시가 좋소야 너무 고생해서 일을 못 해갖고 고생해서   \n",
      "\n",
      "                                                standard  form_len  \n",
      "1      그러니까 며느리가 교회 권사가 돼갖고 안 하려고 하는데 어떻게 집 안에서 이래라 저...        62  \n",
      "2        그랑께 내가 해놓고 제가 와서 기도나 하고 그러라고 가정 편하게 하려고 그러고 삽니다        47  \n",
      "7                                           윷 놀이도 하고 그래요        12  \n",
      "8           뭐 그냥 세 살 네 살 애들도 갖고 할 만한 판이 있더만 뭐판인지 모르는데 있어        44  \n",
      "12     그러니까 될 수 있으면 한 나이도 또 참 그것도 나이가 먹으면 헷갈릴 때도 있고 여...        58  \n",
      "...                                                  ...       ...  \n",
      "36691                                        단풍놀이는 많이 갔지        11  \n",
      "36692               내장산 선운사 관악산 월출산 어 주로 가을에 단풍놀이를 많이 가지        36  \n",
      "36693                          나는 여기 읍에서 이 도시에서 사는 거이 좋소        25  \n",
      "36695                       그러니까 지금은 참 대통령 세상 살고 있소 편안하니        28  \n",
      "36696           그러니까 나는 이 도시가 좋아요  너무 고생해서 일을 못 해갖고 고생해서        40  \n",
      "\n",
      "[23715 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "## 데이터 불러오기\n",
    "df_val = pd.read_csv(\"./중노년층_한국어_방언_데이터/data_2인발화.csv\")\n",
    "\n",
    "## 혹시모를 결측치 제거하기\n",
    "df_val = df_val.dropna(subset=['standard', 'dialect'])\n",
    "\n",
    "## 문장 길이 4-64로 조정하기\n",
    "df_val['form_len'] = df_val.standard.apply(lambda x : len(str(x)))\n",
    "print(df_val)\n",
    "df_val = df_val[(df_val.form_len > 3) & (df_val.form_len <= 64)]\n",
    "print(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c0c3b",
   "metadata": {},
   "source": [
    "### 2. Corpus_BLEU(100문장) 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e46294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 translated\n",
      "sentence 2 translated\n",
      "sentence 3 translated\n",
      "sentence 4 translated\n",
      "sentence 5 translated\n",
      "sentence 6 translated\n",
      "sentence 7 translated\n",
      "sentence 8 translated\n",
      "sentence 9 translated\n",
      "sentence 10 translated\n",
      "sentence 11 translated\n",
      "sentence 12 translated\n",
      "sentence 13 translated\n",
      "sentence 14 translated\n",
      "sentence 15 translated\n",
      "sentence 16 translated\n",
      "sentence 17 translated\n",
      "sentence 18 translated\n",
      "sentence 19 translated\n",
      "sentence 20 translated\n",
      "sentence 21 translated\n",
      "sentence 22 translated\n",
      "sentence 23 translated\n",
      "sentence 24 translated\n",
      "sentence 25 translated\n",
      "sentence 26 translated\n",
      "sentence 27 translated\n",
      "sentence 28 translated\n",
      "sentence 29 translated\n",
      "sentence 30 translated\n",
      "sentence 31 translated\n",
      "sentence 32 translated\n",
      "sentence 33 translated\n",
      "sentence 34 translated\n",
      "sentence 35 translated\n",
      "sentence 36 translated\n",
      "sentence 37 translated\n",
      "sentence 38 translated\n",
      "sentence 39 translated\n",
      "sentence 40 translated\n",
      "sentence 41 translated\n",
      "sentence 42 translated\n",
      "sentence 43 translated\n",
      "sentence 44 translated\n",
      "sentence 45 translated\n",
      "sentence 46 translated\n",
      "sentence 47 translated\n",
      "sentence 48 translated\n",
      "sentence 49 translated\n",
      "sentence 50 translated\n",
      "sentence 51 translated\n",
      "sentence 52 translated\n",
      "sentence 53 translated\n",
      "sentence 54 translated\n",
      "sentence 55 translated\n",
      "sentence 56 translated\n",
      "sentence 57 translated\n",
      "sentence 58 translated\n",
      "sentence 59 translated\n",
      "sentence 60 translated\n",
      "sentence 61 translated\n",
      "sentence 62 translated\n",
      "sentence 63 translated\n",
      "sentence 64 translated\n",
      "sentence 65 translated\n",
      "sentence 66 translated\n",
      "sentence 67 translated\n",
      "sentence 68 translated\n",
      "sentence 69 translated\n",
      "sentence 70 translated\n",
      "sentence 71 translated\n",
      "sentence 72 translated\n",
      "sentence 73 translated\n",
      "sentence 74 translated\n",
      "sentence 75 translated\n",
      "sentence 76 translated\n",
      "sentence 77 translated\n",
      "sentence 78 translated\n",
      "sentence 79 translated\n",
      "sentence 80 translated\n",
      "sentence 81 translated\n",
      "sentence 82 translated\n",
      "sentence 83 translated\n",
      "sentence 84 translated\n",
      "sentence 85 translated\n",
      "sentence 86 translated\n",
      "sentence 87 translated\n",
      "sentence 88 translated\n",
      "sentence 89 translated\n",
      "sentence 90 translated\n",
      "sentence 91 translated\n",
      "sentence 92 translated\n",
      "sentence 93 translated\n",
      "sentence 94 translated\n",
      "sentence 95 translated\n",
      "sentence 96 translated\n",
      "sentence 97 translated\n",
      "sentence 98 translated\n",
      "sentence 99 translated\n",
      "sentence 100 translated\n",
      "방언 -> 표준어 번역 BLEU Score : 0.7643999092483404\n",
      "표준어 -> 방언 번역 BLEU Score : 0.6230947510479574\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import pipeline\n",
    "\n",
    "# 번역을 위한 파이프라인 생성하기\n",
    "translation_pipeline = pipeline(\n",
    "    \"translation_xx_to_yy\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    "    max_length = 64\n",
    ")\n",
    "\n",
    "dialect_token = \"[방언]\"\n",
    "standard_token = \"[표준]\"\n",
    "\n",
    "# 모든 번역을 저장할 리스트\n",
    "dialect_targets = []\n",
    "standard_targets = []\n",
    "dialect_predictions = []\n",
    "standard_predictions = []\n",
    "i = 1\n",
    "\n",
    "for index, row in df_val.head(100).iterrows():\n",
    "    dialect = row['dialect']\n",
    "    standard = row['standard']\n",
    "\n",
    "    ## 방언 -> 표준어 번역 수행\n",
    "    predicted_sentence_dialect = translation_pipeline(dialect_token + \" \" + dialect)[0]['translation_text']\n",
    "\n",
    "    ## 표준어 -> 방언 번역 수행\n",
    "    predicted_sentence_standard = translation_pipeline(standard_token + \" \" + standard)[0]['translation_text']\n",
    "\n",
    "    # 토크나이저를 사용하여 문장을 토큰화\n",
    "    dialect_tokens = tokenizer.tokenize(dialect)\n",
    "    standard_tokens = tokenizer.tokenize(standard)\n",
    "    predicted_tokens_dialect = tokenizer.tokenize(predicted_sentence_dialect)\n",
    "    predicted_tokens_standard = tokenizer.tokenize(predicted_sentence_standard)\n",
    "\n",
    "    # 토큰화된 문장을 리스트에 추가\n",
    "    dialect_targets.append([dialect_tokens])  # 참조는 리스트의 리스트가 되어야 함\n",
    "    standard_targets.append([standard_tokens])  # 참조는 리스트의 리스트가 되어야 함\n",
    "    dialect_predictions.append(predicted_tokens_dialect)\n",
    "    standard_predictions.append(predicted_tokens_standard)\n",
    "    print(f'sentence {i} translated')\n",
    "    i += 1\n",
    "\n",
    "# corpus_bleu 함수를 사용한 BLEU 점수 계산\n",
    "from_dialect_to_standard_bleu_score = corpus_bleu(standard_targets, dialect_predictions)\n",
    "from_standard_to_dialect_bleu_score = corpus_bleu(dialect_targets, standard_predictions)\n",
    "print(f\"방언 -> 표준어 번역 BLEU Score : {from_dialect_to_standard_bleu_score}\")\n",
    "print(f\"표준어 -> 방언 번역 BLEU Score : {from_standard_to_dialect_bleu_score}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64be58",
   "metadata": {},
   "source": [
    "### 3. 단일 문장 번역 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8f9851a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sentence : 빨래를 얼마나 한다고 저런 것을 사버렸어\n",
      "표준어 -> 방언: 빨래를 을매나 한다고 저런 것을 사부렀으\n",
      "\n",
      "input_sentence : 빨래를 을매나 헌다고 저런 것을 사부렀어\n",
      "방언 -> 표준어: 빨래를 얼마나 한다고 저런 것을 사버렸어\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 번역을 위한 파이프라인 생성\n",
    "translation_pipeline = pipeline(\n",
    "    \"translation_xx_to_yy\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "dialect_token = \"[방언]\"\n",
    "standard_token = \"[표준]\"\n",
    "\n",
    "# 특정 문장 번역 예시\n",
    "input_sentence = \"빨래를 얼마나 한다고 저런 것을 사버렸어\"  # 예시 입력 문장\n",
    "translated_sentence = translation_pipeline(standard_token + \" \" + input_sentence)[0]['translation_text']\n",
    "print('input_sentence :', input_sentence)\n",
    "print(\"표준어 -> 방언:\", translated_sentence)\n",
    "print()\n",
    "\n",
    "input_sentence = \"빨래를 을매나 헌다고 저런 것을 사부렀어\"  # 예시 입력 문장\n",
    "translated_sentence = translation_pipeline(dialect_token + \" \" + input_sentence)[0]['translation_text']\n",
    "print('input_sentence :', input_sentence)\n",
    "print(\"방언 -> 표준어:\", translated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
